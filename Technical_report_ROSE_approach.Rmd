---
title: "Building Bank Product Recommendation System"
author: "Karine Vasilyan"
date: "December, 2017"
output:
  word_document: default
  pdf_document: default
---

# Project Overview

*Scenario 2(Using ROSE package to solve imbalace problem)*

In this project, I develop a collaborative filtering recommender (CFR) system for recommending products to bank customers. 

The basic idea of CFR systems is that, if two users share the same interests in the past, e.g. they liked the same book or the same product, they will also have similar tastes in the future. If, for example, user A and user B have a similar purchase history and user A recently bought a book that user B has not yet seen, the basic idea is to propose this book to user B.

The collaborative filtering approach considers only user preferences and does not take into account the features or contents of the items (books or products) being recommended. In this project, in order to recommend bank products I will use a data set from Kaggle, and is publicly available at
https://www.kaggle.com/c/santander-product-recommendation/data

##Used Libraries

The following libraries were used in this project:

```{r libs, warning=FALSE, error=FALSE, message=FALSE}

library(data.table)
library(ggplot2)

library(readr)
library(reshape2)
library(corrplot)

library(Matrix)
library(arules)
library(recommenderlab)

library(lattice) 
library(ROSE)

```
## Dataset

The Whole dataset has a 13,647,309 obsarvation and 48 attributes. In order to keep the recommender simple, in this part I used only customer codes and products information from the dataset which is only 25 attributes.

```{r data_load, warning=FALSE, error=FALSE, echo=FALSE}

# Reading data from csv file

df<-fread("C:/Users/Hrant/Documents/Kara/Data Science/Capstone Project/train_ver2.csv",select = c('ncodpers', 'ind_ahor_fin_ult1', 'ind_aval_fin_ult1', 'ind_cco_fin_ult1', 'ind_cder_fin_ult1', 'ind_cno_fin_ult1', 'ind_ctju_fin_ult1','ind_ctma_fin_ult1', 'ind_ctop_fin_ult1', 'ind_ctpp_fin_ult1',   'ind_deco_fin_ult1', 'ind_deme_fin_ult1', 'ind_dela_fin_ult1','ind_ecue_fin_ult1', 'ind_fond_fin_ult1', 'ind_hip_fin_ult1','ind_plan_fin_ult1', 'ind_pres_fin_ult1', 'ind_reca_fin_ult1','ind_tjcr_fin_ult1', 'ind_valo_fin_ult1', 'ind_viv_fin_ult1',   'ind_nomina_ult1', 'ind_nom_pens_ult1', 'ind_recibo_ult1'),showProgress =F) 

# Let's rename colunms name, make more understandable

colnames(df)<- c("Cust_code",
                 "Acc_Saving", 
"Acc_Guarantees", 
"Acc_Current", 
"Acc_Derivada",
"Acc_Payroll",	
"Acc_Junior",
"Acc_Más_ particular",
"Acc_Particular",
"Acc_Particular_Plus",
"Acc_Short_term_deposits",	
"Acc_Medium_term_deposits", 
"Acc_Long_term_deposits", 
"Acc_e-account",	
"Acc_Funds", 
"Acc_Mortgage",	
"Acc_plan_Pensions", 
"Acc_Loans", 
"Acc_Taxes", 
"Acc_Credit_Card", 
"Acc_Securities", 
"Acc_Home", 
"Acc_nom_Payroll", 
"Acc_nom_Pensions", 
"Acc_Direct_Debit")
```

Let's review our data

### Let's review our data

```{r head , warning=FALSE, error=FALSE, echo=FALSE }

head(df)

```


```{r str, warning=FALSE, error=FALSE, echo=FALSE}

str(df)

```


```{r summary, warning=FALSE, error=FALSE, echo=FALSE}

summary(df)

```

Our data consist of Customer code (Cust_code) and 24 products/Accounts which are binary values. According to the documentation,if customer doesn't use this Account it assigned 0 value otherwise 1.

## Exploring and Cleaning Data


#### Let's check, are there any missing value 

```{r na, warning=FALSE, error=FALSE}

sapply(df,function(x)any(is.na(x)))

```

We see that some Accounts have *NA* values 

### Let's repleace with 0 values and at the same time remove duplicates


```{r rm_NA&duplicates, warning=FALSE, error=FALSE}

df[is.na(df)]<-as.integer(0)

df<-df[!duplicated(df),]

paste0("Number of customers in dataset: ",length(df$Cust_code))

# Now let's check Cust_code attributes are unique or we have repetitions

paste0("Number of unique customers in dataset: ",length(unique(df$Cust_code)))

```

We see that Customer code is not unique. To make it unique I combine all information into one line for each customer.

```{r unique_Custcode, warning=FALSE, error=FALSE, echo=FALSE}

a<- melt(df, id.vars="Cust_code" )
df.unique<-dcast(a, Cust_code~variable, sum)
df.unique[,2:25]<-as.data.frame(ifelse(df.unique[,2:25]>0, 1, 0)) # Binarize our product attributes

```

### Now, let's visualize to see any correlation

```{r corr, warning=FALSE, error=FALSE, echo=FALSE}

product_corr <- cor(df.unique[,2:25])
corrplot(product_corr, method="square", tl.cex = 0.7)

```

We can see actual correlation values. Acc_nom_Pensions and Acc_nom_Payroll and Acc_Payroll products are the ones are highly correlated. Since I don't have detailed description of each product, I cannot assume the reasons why they're highly correlated.

```{r num_corr, warning=FALSE, error=FALSE, echo=FALSE}

layout(matrix(1:1, ncol = 4))
corrplot(product_corr, method="number", tl.cex = 1)

```

### Number of products that customers have

```{r total_product, warning=FALSE, error=FALSE, echo=FALSE}

df.unique$totalproducts<-rowSums(df.unique[,2:25], na.rm = T)

table(df.unique$totalproducts)

barplot(table(df.unique$totalproducts),main = "Total number of products customers use",  xlab="Number of Products", las=1.5)

```

Around 225,000 customers, which is 24% of total number of Customers, don't have a single product, and around 45% of the customers own only one product. At most, there are only 15 products that one person holds at the same time.


### Number of users of the top accounts


```{r topAccounts, warning=FALSE, error=FALSE, echo=FALSE}

#Now, let's see which product is most popular.

num_per_acc <- colSums(df.unique[,-c(1,26)], na.rm = T ) # count for each account 

table_acc <- data.frame(account = names(num_per_acc), num = num_per_acc) # create dataframe of accounts
table_acc <- table_acc[order(table_acc$num, 
                                 decreasing = TRUE), ] # sort by number of accounts

table_acc
table_acc_percent<-table_acc
table_acc_percent[,2]<-round(table_acc[,2]*100/sum(table_acc[,2]),1)

ggplot(table_acc_percent, aes(reorder(account, -num),num)) +
  geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 40, hjust = 1), axis.title.x = element_blank(),  axis.title.y = element_blank())+ggtitle("Percentage of users of the Accounts")

ggplot(table_acc, aes(reorder(account, -num),num)) +
  geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 40, hjust = 1), axis.title.x = element_blank(),  axis.title.y = element_blank())+ggtitle("Number of users of the Accounts")

paste0("Difference of top 1st and top 2nd accounts:  ",table_acc[1,2]-table_acc[2,2])

```
We see that "Acc_Current" is the most popular account, around 42% of customers use it, and it exceed the "Acc_Direct_Debit" account, which is around 11%, by 465,183.


### Delete all users that don't own any products


```{r delete_zero, warning=FALSE, error=FALSE, echo=FALSE}

df.clean<-subset(df.unique, df.unique$totalproducts>0)

prodname<-c("Cust_code", as.vector(table_acc[,1]))


df.clean<- subset(df.clean, select= prodname) 

```

### Distribution of the values

Let's see how are distributed binary values in our dataset

```{r distrib, warning=FALSE, error=FALSE, echo=FALSE}

vec<- melt(df.clean, id.vars="Cust_code" )

round(table(vec$value)*100/sum(table(vec$value)),0)

qplot(as.factor(vec$value), main="Distribution of the values")+ geom_bar() +labs(x="value")

```
We see that our dataset is imbalanced, 92 : 8 ratio, where 92% of dataset has a 0 value and 8% - 1 value

In order to resolve imbalance problem, in this part I use ROSE(Random Over Sampling Examples) package, which helps to generate artificial data based on sampling methods, to overcome this problem.

So, let's see the distridution again.

```{r unbalance_resampling, warning=FALSE, error=FALSE, echo=FALSE}

vec$Cust_code<-as.factor(vec$Cust_code)

resample <- ROSE(value ~ ., data = vec, seed = 1)$data

round(table(resample$value)*100/sum(table(resample$value)),0)

df.Rose<-dcast(resample, Cust_code~variable)

df.Rose[,2:25]<-as.data.frame(ifelse(df.Rose[,2:25]>0, 1, 0))
df.Rose$Cust_code<-as.numeric(as.character(df.Rose$Cust_code))
vec<- melt(df.Rose, id.vars="Cust_code" )

round(table(vec$value)*100/sum(table(vec$value)),0)
 
```

Now we have 53 : 47 ratio, which is more better than before

### Randomly reduce dataset size and Create rating matrix

Before building Recommender system  I only randomly select 10000 users since the large dataset makes challenging to use user-based collaborative filtering

```{r rat_mat, warning=FALSE, error=FALSE, echo=FALSE}

rat<-df.Rose[sample(nrow(df.Rose),10000),] #randomly select 10000 users

#Create ratings matrix

ratingmat <- as.matrix(rat[,-1]) #remove Cust_code 

rownames(ratingmat)<-rat[,1]

#Convert rating matrix into a recommenderlab sparse matrix

ratingmat <- as(ratingmat, "binaryRatingMatrix")
ratingmat

head(as(ratingmat,"matrix"))

```

## Exploring Parameters of Recommendation Models

The *recommenderlab* package contains some options for the recommendation algorithm:

```{r rec_overview, warning=FALSE, error=FALSE, echo=FALSE}
recommender_models <- recommenderRegistry$get_entries(dataType = "binaryRatingMatrix")
names(recommender_models)
lapply(recommender_models, "[[", "description")
```

I will use IBCF and UBCF models. Check the parameters of these two models.

```{r model_param, warning=FALSE, error=FALSE}
recommender_models$IBCF_binaryRatingMatrix
recommender_models$UBCF_binaryRatingMatrix

```


## Exploring Similarity Data

Collaborative filtering algorithms are based on measuring the similarity between
users or between items. For this purpose, *recommenderlab* contains the similarity
function. The supported methods to compute similarities are *cosine, pearson*,
and *jaccard*.

Next, I determine how similar the first four users are with each other by creating and visualizing similarity matrix that uses the *jaccard* distance:

```{r sim_users, warning=FALSE, error=FALSE, echo=FALSE}
similarity_users <- similarity(ratingmat[1:4, ], 
                               method = "jaccard", 
                               which = "users")
as.matrix(similarity_users)


levelplot(as.matrix(similarity_users),main = "User similarity",ylab=" ", xlab=" ", col.regions=colorRampPalette(c("red","yellow","white"),space="rgb") )
```

In the given matrix, each row and each column corresponds to a user, and each cell corresponds to the similarity between two users. The more red the cell is, the more similar two users are. Note that the diagonal is red, since it's comparing each user with itself.

Using the same approach, I compute similarity between the first four products.

```{r sim_prod, warning=FALSE, error=FALSE, echo=FALSE}
similarity_items <- similarity(ratingmat[, 1:4], method =
                                 "jaccard", which = "items")
as.matrix(similarity_items)

levelplot(as.matrix(similarity_items),main = "Products similarity",ylab=" ", xlab=" ", col.regions=colorRampPalette(c("red","yellow","white"),space="rgb"),scales=list(x=list(rot=30, cex=0.7)) )

```

### Heatmap of the rating matrix

I visualize the whole matrix of ratings by building a heat map. Each row of the matrix corresponds to a user, each column to a product, and each cell to its rating.

```{r heat_rate, warning=FALSE, error=FALSE, echo=FALSE}
image(ratingmat[1:1000,],main = "Heatmap of the rating matrix",useraster=T) # hard to read-too many dimensions

image(ratingmat[1:20, 1:24], main = "Heatmap of the first 20 rows and 24 columns")
      
```

Since there are too many users, the first chart is hard to read. The second chart is built zooming in on the first rows and columns.

We can see that some users have more accounts than the others and there are some products that are not so popular.

## ITEM-based Collaborative Filtering Model

Collaborative filtering is a branch of recommendation that takes account of the information about different users. The word "collaborative" refers to the fact that users collaborate with each other to recommend items. In fact, the algorithms take account of user ratings and preferences.

The starting point is a rating matrix in which rows correspond to users and columns correspond to items. The core algorithm is based on these steps:

1. For each two items, measure how similar they are in terms of having received similar preference by similar users
2. For each item, identify the k most similar items
3. For each user, identify the items that are most similar to the user's purchases

### Defining training/test sets

I build the model using 80% of the whole dataset as a training set, and 20% - as a test set. 

```{r train_test_sets, warning=FALSE, message=FALSE, echo=FALSE}
which_train <- sample(x = c(TRUE, FALSE), 
                      size = nrow(ratingmat),
                      replace = TRUE, 
                      prob = c(0.8, 0.2))

recc_data_train <- ratingmat[which_train, ]
recc_data_test <- ratingmat[!which_train, ]

```

### Building the recommendation model IBCF

Let's have a look at the default parameters of IBCF model. Here, *k* is the number of items to compute the similarities among them in the first step. After, for each item, the algorithm identifies its *k* most similar items and stores the number. *method* is a similarity funtion, which is *Jaccard* by default, may also be *Cosine*. I create the model using the default parameters of method = Jaccard and k=30.

```{r build_recommenderIBCF, warning=FALSE, message=FALSE, echo=FALSE}
recommender_models <- recommenderRegistry$get_entries(dataType ="binaryRatingMatrix")
recommender_models$IBCF_binaryRatingMatrix$parameters

recc_model <- Recommender(data = recc_data_train, 
                          method = "IBCF",
                          parameter = list(k = 30,normalize_sim_matrix=TRUE))

recc_model
class(recc_model)
```

### Applying recommender system on the dataset:

Now, it is possible to recommend products to the users in the test set.

For each user, the algorithm extracts its used products. For each product, it identifies all its similar items, starting from the similarity matrix. Then, the algorithm ranks each similar item in this way:

* Extract the user rating of each purchase associated with this item. The rating is used as a weight.
* Extract the similarity of the item with each purchase associated with this item.
* Multiply each weight with the related similarity. 
* Sum everything up.

Then, the algorithm identifies the top 3 recommendation:

```{r apply_IBCF, warning=FALSE, message=FALSE, echo=FALSE}

recc_predicted <- predict(object = recc_model, 
                          newdata = recc_data_test, 
                         n=3)
recc_predicted
```

Let's explore the results of the recommendations for the first user:

```{r explore_res_IBCF, warning=FALSE, message=FALSE, echo=FALSE}

recc_user_1 <- recc_predicted@items[[1]] # recommendation for the first user
item_user_1 <- recc_predicted@itemLabels[recc_user_1]

item_user_1

```

It's also possible to define a matrix with the recommendations for each user. I visualize the recommendations for the first ten users:

```{r recc_matrix, warning=FALSE, message=FALSE, echo=FALSE}
recc_matrix <- sapply(recc_predicted@items, function(x){colnames(ratingmat)[x] }) # matrix with the recommendations for each user


as.matrix(recc_matrix)[1:4,]
```

IBCF recommends items on the basis of the similarity matrix. It's an eager-learning model, that is, once it's built, it doesn't need to access the initial data. For each item, the model stores the k-most similar, so the amount of information is small once the model is built. This is an advantage in the presence of lots of data.


```{r times_per_products_IBCF, warning=FALSE, message=FALSE, echo=FALSE}

#number_of_items <- table(recc_matrix)

#ggplot(as.data.frame(number_of_items), aes(reorder(recc_matrix, -Freq),Freq)) +geom_bar(stat="identity") + 
#  theme(axis.text.x = element_text(angle = 40, hjust = 1), axis.title.x = element_blank(),  axis.title.y = element_blank())+ggtitle("Distribution of the number of items for IBCF")

```
We see that according IBCF recommender system "Acc_Pensions" and "Acc_Payroll" are the most often reccomended items.


In addition, this algorithm is efficient and scalable, so it works well with big rating matrices.

## USER-based Collaborative Filtering Model


Now, I will use the user-based approach. According to this approach, given a new user, its similar users are first identified. Then, the top-rated items rated by similar users are recommended. 

For each new user, these are the steps:

1. Measure how similar each user is to the new one. Like IBCF, popular similarity measures are jaccard, correlation, and cosine.
2. Identify the most similar users. The options are:

   * Take account of the top k users (k-nearest_neighbors)
   * Take account of the users whose similarity is above a defined threshold
   
3. Rate the items rated by the most similar users. The rating is the average
rating among similar users and the approaches are:

   * Average rating
   * Weighted average rating, using the similarities as weights
   
4. Pick the top-rated items.


### Building the recommendation system UBCF:

Again, let's first check the default parameters of UBCF model. Here, *nn* is a number of similar users, and *method* is a similarity function, which is *jaccard* by default. I build a recommender model leaving the parameters to their defaults and using the training set.

```{r build_UBCF, warning=FALSE, message=FALSE, echo=FALSE}
recommender_models <- recommenderRegistry$get_entries(dataType ="binaryRatingMatrix")
recommender_models$UBCF_binaryRatingMatrix$parameters

recc_model_u <- Recommender(data = recc_data_train, method = "UBCF", list(k = 30,normalize_sim_matrix=TRUE))
recc_model_u

#names(getModel(recc_model_u))
getModel(recc_model_u)$data
```

### Applying the recommender model on the test set

In the same way as the IBCF, now I determine the top tree recommendations for each new user in the test set. 

```{r apply_UBCF, warning=FALSE, message=FALSE, echo=FALSE}

recc_predicted_u <- predict(object = recc_model_u,
                          newdata = recc_data_test, 
                          n = 3) 
recc_predicted_u
```

Let's take a look at the first user result:

```{r explore_UBCF, warning=FALSE, message=FALSE, echo=FALSE}

recc_user_u1 <- recc_predicted_u@items[[1]] # recommendation for the first user
item_user_u1 <- recc_predicted_u@itemLabels[recc_user_u1]

item_user_u1

```
Also the first ten users:

```{r explore_UBCF_1, warning=FALSE, message=FALSE, echo=FALSE}
recc_matrix_u <- sapply(recc_predicted_u@items, 
                      function(x){ colnames(ratingmat)[x] })

recc_matrix_u[, 1:10]

```

The above matrix contain products name (rows) for the first ten users (columns) in our test dataset.

I also compute how many times each products got recommended and build the related frequency histogram:

```{r times_per_products_UBCF, warning=FALSE, message=FALSE, echo=FALSE}
number_of_items <- table(recc_matrix_u)

ggplot(as.data.frame(number_of_items), aes(reorder(recc_matrix_u, -Freq),Freq)) +geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 40, hjust = 1), axis.title.x = element_blank(),  axis.title.y = element_blank())+ggtitle("Distribution of the number of items for UBCF")

```
We see that UBCF recommender system "Acc_Direct_Debit", "Acc_nom_Pensions" and "Acc_Particular" are most often reccomended items.


## Evaluating the Recommender Systems

There are a few options to choose from when deciding to create a recommendation engine. In order to compare their performances and choose the most appropriate model, I follow these steps:

* Prepare the data to evaluate performance
* Evaluate the performance of some models
* Choose the best performing models
* Optimize model parameters

### Preparing the data to evaluate models

We need two trainig and testing data to evaluate the model. There are several methods to create them: 1) splitting the data into training and test sets, 2) bootstrapping, 3) using k-fold.

### Evavluating the ratings

I use the k-fold cross-validation approach for evaluation. This approach is the most accurate one, although it's computationally heavier. 

Using k-fold cross-validation approach, we split the data into some chunks, take a chunk out as the test set, and evaluate the accuracy. Then, we can do the same with each other chunk and compute the average accuracy.


```{r eval_ratings, message=FALSE, warning=FALSE, echo=FALSE}

eval_sets <- evaluationScheme(data = ratingmat, 
                              method = "cross-validation",
                              k = 4, 
                              given = 3,
                              goodRating=1)
size_sets <- sapply(eval_sets@runsTrain, length)
size_sets

eval_recommender <- Recommender(data = getData(eval_sets, "train"),
                                method = "IBCF", 
                                parameter = NULL)

eval_recommender_u <- Recommender(data = getData(eval_sets, "train"),
                                method = "UBCF", 
                                parameter = NULL)

eval_prediction <- predict(object = eval_recommender, 
                           newdata = getData(eval_sets, "known"), 
                           n = 3, 
                           type = "topNList")


eval_prediction_u <- predict(object = eval_recommender_u, 
                           newdata = getData(eval_sets, "known"), 
                           n = 3, 
                           type = "topNList")


```
Using 4-fold approach, we get four sets of the same size 7500.


Now, I compute the accuracy measures for each user and visualize F-measure for each model IBCF and UBCF.

```{r acc, message=FALSE,  warning=FALSE, echo=FALSE}

eval_accuracy <- calcPredictionAccuracy(x = eval_prediction, 
                                        data = getData(eval_sets, "unknown"),byUser=T, given=6)

eval_accuracy_u <- calcPredictionAccuracy(x = eval_prediction_u, 
                                        data = getData(eval_sets, "unknown"), byUser=T, given=6)


head(eval_accuracy)

head(eval_accuracy_u)

qplot((2*eval_accuracy[, "recall"]*eval_accuracy[, "precision"])/(2*eval_accuracy[, "recall"]+eval_accuracy[, "precision"])) + 
  geom_histogram(binwidth = 0.1) + xlab("F-measure")+
  ggtitle("Distribution of the F-measure by user for IBCF")


qplot((2*eval_accuracy_u[, "recall"]*eval_accuracy_u[, "precision"])/(2*eval_accuracy_u[, "recall"]+eval_accuracy_u[, "precision"])) + 
  geom_histogram(binwidth = 0.1) + xlab("F-measure")+
  ggtitle("Distribution of the F-measure by user for UBCF")


```

In order to have a performance index for the whole model, I specify *byUser* as FALSE and compute the average indices:

```{r acc_IBCF, message=FALSE,  warning=FALSE, echo=FALSE}

eval_accuracy <- calcPredictionAccuracy(x = eval_prediction, 
                                        data = getData(eval_sets, "unknown"), 
                                        byUser = F,given=3) 
eval_accuracy["F.measure"]<-2*eval_accuracy[5]*eval_accuracy[6]/(eval_accuracy[5]+eval_accuracy[6])

cat("IBCF: " ,"\n")

eval_accuracy

eval_accuracy_u <- calcPredictionAccuracy(x = eval_prediction_u, 
                                        data = getData(eval_sets, "unknown"), 
                                        byUser = F,given=3) 
eval_accuracy_u["F.measure"]<-2*eval_accuracy_u[5]*eval_accuracy_u[6]/(eval_accuracy_u[5]+eval_accuracy_u[6])

cat( "\n", "UBCF: ", "\n")


eval_accuracy_u


```

The measures of accuracy are useful to compare the performance of different models on the same data.

## Evaluating the recommendations

Another way to measure accuracies is by comparing the recommendations with
the purchases having a positive rating. For this, I can make use of a prebuilt
*evaluate* function in *recommenderlab* library. The function evaluate the recommender performance depending on the number *n* of items to recommend to each user. I use *n* as a sequence n = seq(1, 10, 1). The first rows of the resulting performance matrix is presented below:

```{r eval_recomms, message=FALSE, warning=FALSE, echo=FALSE}

results <- evaluate(x = eval_sets, 
                    method = "UBCF", 
                    n = seq(1, 10, 1))
getConfusionMatrix(results)[[1]]

```

In order to have a look at all the splits at the same time, I sum up the indices of columns TP, FP, FN and TN:

```{r conf_matrix_whole, message=FALSE, warning=FALSE, echo=FALSE}
columns_to_sum <- c("TP", "FP", "FN", "TN")

indices_summed <- Reduce("+", getConfusionMatrix(results))[, columns_to_sum]

indices_summed

```

Finally, I plot the ROC,  the precision/recall, and F-measure curves:

```{r roc, message=FALSE, warning=FALSE}

plot(results, annotate = TRUE, main = "ROC curve")

plot(results, "prec/rec", annotate = TRUE, main = "Precision-recall")

cfm<-as.data.frame(getConfusionMatrix(results)[[1]])

cfm$F.measure<-2*cfm$precision*cfm$recall/(cfm$precision+cfm$recall)

plot(cfm$precision, type="o", col=2,xlab = "number of items reccomended", ylab="index",ylim=c(0,1), main = "Precision, Recall, F-measure")
lines(cfm$recall,type="o", col=3)

lines(cfm$F.measure,type="o", col=4)
legend("topleft", legend = c("Precision", "Recall", "F-measure"), col=2:4, pch=1)


```

If a high number of products are recommended, the precision decreases. On the other hand, the higher percentage of products are recommended the higher is the recall.

## Comparing models

In order to compare different models, I define them as a following list:

* Item-based collaborative filtering, using the Cosine as the distance function
* Item-based collaborative filtering, using the Jaccard as the distance function
* Item-based collaborative filtering, using the Pearson correlation as the distance function
* User-based collaborative filtering, using the Cosine as the distance function
* User-based collaborative filtering, using the Jaccard as the distance function
* User-based collaborative filtering, using the Pearson correlation as the distance function
* Random recommendations to have a base line
* Popular recommendations to have a base line

```{r define_diff_models, warning=FALSE, message=FALSE, echo=FALSE}

models_to_evaluate <- list(
IBCF_cos = list(name = "IBCF", 
                param = list(method = "cosine")),
IBCF_jac = list(name = "IBCF", 
                param = list(method = "jaccard")),
IBCF_pear = list(name = "IBCF", 
                param = list(method = "pearson")),
UBCF_cos = list(name = "UBCF", 
                param = list(method = "cosine")),
UBCF_jac = list(name = "UBCF", 
                param = list(method = "jaccard")),
UBCF_pear = list(name = "UBCF", 
                param = list(method = "pearson")),
Random = list(name = "Random", 
                param = NULL),
Popular = list(name = "Popular", 
                param = NULL)
)
```

Then, I define a different set of numbers for recommended products (n_recommendations <- seq(1, 10, 1)), run and evaluate the models:

```{r params, warning=FALSE, message=FALSE, echo=FALSE}

n_recommendations <-seq(1, 10, 1)
list_results <- evaluate(x = eval_sets, 
                         method = models_to_evaluate, 
                         n = n_recommendations)

sapply(list_results, class) == "evaluationResults"

```


## Identifying the most suitable model

I compare the models by building a chart displaying their ROC curves and recision/recall curves.

```{r compare_models_roc, message=FALSE, warning=FALSE, echo=FALSE}
plot(list_results, annotate = 1, legend = "bottomright") 
title("ROC curve")

plot(list_results, "prec/rec", annotate = 1, legend = "topleft" )
title("Precision-recall")
```

According to the Precision/Recall chart we can see that Popular model gives higher results amonge the others.

Therefore, I choose UBCF with jaccard distance as a model. Depending on what is the main purpose of the system, an appropriate number of items to recommend should be defined.


The following table presents as an example of the performance evaluation matrix for the UBCF with Jaccard distance:


```{r ex_compare, warning=FALSE, message=FALSE, echo=FALSE}

avg_matrices <- lapply(list_results, avg)

cfm_avg<-avg_matrices$UBCF_jac[,5:6]

F.measure<-2*cfm_avg[,"precision"]*cfm_avg[,"recall"]/(cfm_avg[,"precision"]+cfm_avg[,"recall"])
cfm_avg<-cbind(cfm_avg, F.measure)

cfm_avg

plot(cfm_avg[,"precision"], type="o", col=2,xlab= "number of items reccomended", ylab="index",ylim=c(0,1), main = "Precision, Recall, F-measure for UBCF method Jaccard distance")
lines(cfm_avg[,"recall"],type="o", col=3)

lines(cfm_avg[,"F.measure"],type="o", col=4)
legend("topleft", legend = c("Precision", "Recall", "F-measure"), col=2:4, pch=1)

```
