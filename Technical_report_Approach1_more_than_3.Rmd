---
title: "Building Bank Product Recommendation System"
author: "Karine Vasilyan"
date: "November 2017"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

# Project Overview

*Scenario 1(Remoing customers who use less than 4 prducts)*

In this project, I develop a collaborative filtering recommender (CFR) system for recommending products to bank customers. 

The basic idea of CFR systems is that, if two users share the same interests in the past, e.g. they liked the same item or the same product, they will also have similar preferences  in the future. If, for example, user x and user y have a similar purchase history and user x recently bought a product that user x has not yet seen, the basic idea is to propose this product to user y.

The collaborative filtering approach considers only user preferences and does not take into account the features or contents of the items (items or products) being recommended. In this project, in order to recommend bank products I will use part of the data set from Kaggle, and is publicly available at
https://www.kaggle.com/c/santander-product-recommendation/data

##Used Libraries

The following libraries were used in this project:

```{r libs, warning=FALSE, error=FALSE, message=FALSE}

library(data.table)
library(ggplot2)

library(readr)
library(reshape2)
library(corrplot)

library(Matrix)
library(arules)
library(recommenderlab)

library(lattice) 
library(ROSE)

```
## Dataset

The Whole data set has a 13,647,309 observation and 48 attributes. In order to keep the recommender simple, in this part I used only customer codes and products information from the data set which is only 25 attributes.

```{r data_load, warning=FALSE, error=FALSE, echo=FALSE}

# Reading data from csv file

df<-fread("C:/Users/Hrant/Documents/Kara/Data Science/Capstone Project/train_ver2.csv",select = c('ncodpers', 'ind_ahor_fin_ult1', 'ind_aval_fin_ult1', 'ind_cco_fin_ult1', 'ind_cder_fin_ult1', 'ind_cno_fin_ult1', 'ind_ctju_fin_ult1','ind_ctma_fin_ult1', 'ind_ctop_fin_ult1', 'ind_ctpp_fin_ult1',   'ind_deco_fin_ult1', 'ind_deme_fin_ult1', 'ind_dela_fin_ult1','ind_ecue_fin_ult1', 'ind_fond_fin_ult1', 'ind_hip_fin_ult1','ind_plan_fin_ult1', 'ind_pres_fin_ult1', 'ind_reca_fin_ult1','ind_tjcr_fin_ult1', 'ind_valo_fin_ult1', 'ind_viv_fin_ult1',   'ind_nomina_ult1', 'ind_nom_pens_ult1', 'ind_recibo_ult1'),showProgress =F) 

# Let's rename colunms name, make more understandable

colnames(df)<- c("Cust_code",
                 "Acc_Saving", 
"Acc_Guarantees", 
"Acc_Current", 
"Acc_Derivada",
"Acc_Payroll",	
"Acc_Junior",
"Acc_Más_ particular",
"Acc_Particular",
"Acc_Particular_Plus",
"Acc_Short_term_deposits",	
"Acc_Medium_term_deposits", 
"Acc_Long_term_deposits", 
"Acc_e-account",	
"Acc_Funds", 
"Acc_Mortgage",	
"Acc_plan_Pensions", 
"Acc_Loans", 
"Acc_Taxes", 
"Acc_Credit_Card", 
"Acc_Securities", 
"Acc_Home", 
"Acc_nom_Payroll", 
"Acc_nom_Pensions", 
"Acc_Direct_Debit")
```

Let's review our data

### Let's review our data

```{r head , warning=FALSE, error=FALSE, echo=FALSE }

head(df)

```


```{r str, warning=FALSE, error=FALSE, echo=FALSE}

str(df)

```


```{r summary, warning=FALSE, error=FALSE, echo=FALSE}

summary(df)

```

Our data consist of Customer code (Cust_code) and 24 products/Accounts which are binary values. According to the documentation,if customer doesn't use this Account it assigned 0 value otherwise 1.

## Exploring and Cleaning Data


#### Let's check, are there any missing value 

```{r na, warning=FALSE, error=FALSE}

sapply(df,function(x)any(is.na(x)))

```

We see that some Accounts have *NA* values 

### Let's replace with 0 values and at the same time remove duplicates


```{r rm_NA&duplicates, warning=FALSE, error=FALSE}

df[is.na(df)]<-as.integer(0)

df<-df[!duplicated(df),]

paste0("Number of customers in dataset: ",length(df$Cust_code))

# Now let's check Cust_code attributes are unique or we have repetitions

paste0("Number of unique customers in dataset: ",length(unique(df$Cust_code)))

```

We see that Customer code is not unique. To make it unique I combine all information into one line for each customer.

```{r unique_Custcode, warning=FALSE, error=FALSE, echo=FALSE}

a<- melt(df, id.vars="Cust_code" )
df.unique<-dcast(a, Cust_code~variable, sum)
df.unique[,2:25]<-as.data.frame(ifelse(df.unique[,2:25]>0, 1, 0)) # Binarize our product attributes

```

### Now, let's visualize to see correlation among products

```{r corr, warning=FALSE, error=FALSE, echo=FALSE}

product_corr <- cor(df.unique[,2:25])
corrplot(product_corr, method="square", tl.cex = 0.7)

```

We can see actual correlation values. Acc_nom_Pensions and Acc_nom_Payroll and Acc_Payroll products are the highly correlated. Due to lack of detailed description of each product, I cannot assume the reasons why they're correlated.

```{r num_corr, warning=FALSE, error=FALSE, echo=FALSE}

layout(matrix(1:1, ncol = 4))
corrplot(product_corr, method="number", tl.cex = 1)

```

### Number of products that customers have

```{r total_product, warning=FALSE, error=FALSE, echo=FALSE}

df.unique$totalproducts<-rowSums(df.unique[,2:25], na.rm = T)

table(df.unique$totalproducts)

barplot(table(df.unique$totalproducts),main = "Total number of products customers use",  xlab="Number of Products", las=1.5)

```

Around 225,000 customers, which is 24% of total number of Customers, don't have a single product, and around 45% of the customers own only one product. At most, there are only 15 products that one person holds at the same time.


### Number of users of the top accounts


```{r topAccounts, warning=FALSE, error=FALSE, echo=FALSE}

#Now, let's see which product is most popular.

num_per_acc <- colSums(df.unique[,-c(1,26)], na.rm = T ) # count for each account 

table_acc <- data.frame(account = names(num_per_acc), num = num_per_acc) # create dataframe of accounts
table_acc <- table_acc[order(table_acc$num, 
                                 decreasing = TRUE), ] # sort by number of accounts

table_acc
table_acc_percent<-table_acc
table_acc_percent[,2]<-round(table_acc[,2]*100/sum(table_acc[,2]),1)

ggplot(table_acc_percent, aes(reorder(account, -num),num)) +
  geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 40, hjust = 1), axis.title.x = element_blank(),  axis.title.y = element_blank())+ggtitle("Percentage of users of the Accounts")

ggplot(table_acc, aes(reorder(account, -num),num)) +
  geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 40, hjust = 1), axis.title.x = element_blank(),  axis.title.y = element_blank())+ggtitle("Number of users of the Accounts")

paste0("Difference of top 1st and top 2nd accounts:  ",table_acc[1,2]-table_acc[2,2])

```
We see that "Acc_Current" is the most popular account, around 42% of customers use it, and it exceed the "Acc_Direct_Debit" account, which is around 11%, by 465,183.


### Let's see how are distributed binary values in our dataset

```{r distrib, warning=FALSE, error=FALSE, echo=FALSE}

vec<- melt(df.unique[,1:25], id.vars="Cust_code" )

round(table(vec$value)*100/sum(table(vec$value)),0)

qplot(as.factor(vec$value), main="Distribution of the values")+ geom_bar() +labs(x="value")

```
We see that our dataset is imbalanced, 94 : 6 ratio, where 94% of data set has a 0 value and 6% - 1 value


In order to resolve imbalance problem, in this part I take only customers who use at least four products. So, let's see the distribution again.

```{r imbalance_resampling, warning=FALSE, error=FALSE, echo=FALSE}

df.clean<-subset(df.unique, df.unique$totalproducts>3)

prodname<-c("Cust_code", as.vector(table_acc[,1]))

df.clean<- subset(df.clean, select= prodname) 

vec<- melt(df.clean, id.vars="Cust_code" )

round(table(vec$value)*100/sum(table(vec$value)),0)

qplot(as.factor(vec$value), main="Distribution of the ratings")+ geom_bar() +labs(x="value")

```
Now we have 77 : 23 ratio, which is more better than before


### Randomly reduce dataset size and Create rating matrix

Before building Recommender system  I only randomly select 10000 users since the large data set makes challenging to use user-based collaborative filtering

```{r rat_mat, warning=FALSE, error=FALSE, echo=FALSE}

rat<-df.clean[sample(nrow(df.clean),10000),] #randomly select 10000 users

#Create ratings matrix

ratingmat <- as.matrix(rat[,-1]) #remove Cust_code 

rownames(ratingmat)<-rat[,1]

#Convert rating matrix into a recommenderlab sparse matrix

ratingmat <- as(ratingmat, "binaryRatingMatrix")
ratingmat

head(as(ratingmat,"matrix"))

```

## Exploring Parameters of Recommendation Models

The *recommenderlab* package contains some options for the recommendation algorithm:

```{r rec_overview, warning=FALSE, error=FALSE, echo=FALSE}
recommender_models <- recommenderRegistry$get_entries(dataType = "binaryRatingMatrix")
names(recommender_models)
lapply(recommender_models, "[[", "description")
```

I will use IBCF and UBCF models. Check the parameters of these two models.

```{r model_param, warning=FALSE, error=FALSE}
recommender_models$IBCF_binaryRatingMatrix
recommender_models$UBCF_binaryRatingMatrix

```


## Exploring Similarity Data

Collaborative filtering algorithms measures the similarity between users or between items. There are several similarity measures, but I would like to highlight three of them: *Cosine similarity, Pearson correlation*, and *Jaccard*. The most popular ones are *Pearson correlation and Cosine* 

Here, I identify how similar the first four users are with each other by creating and visualizing similarity matrix that uses the *jaccard* distance:

```{r sim_users, warning=FALSE, error=FALSE, echo=FALSE}
similarity_users <- similarity(ratingmat[1:4, ], 
                               method = "jaccard", 
                               which = "users")
as.matrix(similarity_users)


levelplot(as.matrix(similarity_users),main = "User similarity",ylab=" ", xlab=" ", col.regions=colorRampPalette(c("red","yellow","white"),space="rgb") )
```

In the given matrix, each row and each column corresponds to a user, and each cell corresponds to the similarity between two users. The more red the cell is, the more similar two users are. Note that the diagonal is red, since it's comparing each user with itself.

Using the same approach, I compute similarity between the first four products.

```{r sim_prod, warning=FALSE, error=FALSE, echo=FALSE}
similarity_items <- similarity(ratingmat[, 1:4], method =
                                 "jaccard", which = "items")
as.matrix(similarity_items)

levelplot(as.matrix(similarity_items),main = "Products similarity",ylab=" ", xlab=" ", col.regions=colorRampPalette(c("red","yellow","white"),space="rgb"),scales=list(x=list(rot=30, cex=0.7)) )

```

### Heatmap of the rating matrix

I visualize the first thousand rows of whole rating matrix by building a heat map. Each row of the matrix represent to a user, each column to a product, and each cell to its rating.

```{r heat_rate, warning=FALSE, error=FALSE, echo=FALSE}
image(ratingmat[1:1000,],main = "Heatmap of the rating matrix",useraster=T) # hard to read-too many dimensions

image(ratingmat[1:20, 1:24], main = "Heatmap of the first 20 rows and 24 columns")
      
```

The heatmap is hard to read, because there are too many users. The second heatmap is built zooming in on the first rows and columns.

We can see that some customers have more accounts than the others and there are some accounts that are not so popular.

## ITEM-based Collaborative Filtering Model

Collaborative filtering is a widely used branch of recommendation system. Collaborative filtering approach assume that people with similar preferences will choose or rate things similarly. For generating top-N recommended items we just need user preferences and/or ratings. There are two main categories of Collaborative  Filtering: Memory-based which is a User-Based algorithm, and  Model-Based which is a Item-Based algorithm.

It is basically, based on a rating matrix where the rows represent users and the columns represent items. The Item-Based algorithm implements the following steps:

.	For every two products, measure how similar based on customers preferences
.	For each product, identify the k-most similar product (which reduce size of similarity matrix)
.	For each customer, identify the products that are most similar to the customer's purchases.

### Defining training/test sets

I build the model using 80% of the whole data set as a training set, and 20% - as a test set. 

```{r train_test_sets, warning=FALSE, message=FALSE, echo=FALSE}
which_train <- sample(x = c(TRUE, FALSE), 
                      size = nrow(ratingmat),
                      replace = TRUE, 
                      prob = c(0.8, 0.2))

recc_data_train <- ratingmat[which_train, ]
recc_data_test <- ratingmat[!which_train, ]

```

### Building the recommendation model IBCF

Let's have a look at the default parameters of IBCF model. Here, *k* is the number of items to compute the similarities among them in the first step. After, for each item, the algorithm identifies its *k* most similar items and stores the number. *method* is a similarity function, which is *Jaccard* by default, may also be *Cosine*. I create the model using the default parameters of method = Jaccard and k=24.

```{r build_recommenderIBCF, warning=FALSE, message=FALSE, echo=FALSE}
recommender_models <- recommenderRegistry$get_entries(dataType ="binaryRatingMatrix")
recommender_models$IBCF_binaryRatingMatrix$parameters

recc_model <- Recommender(data = recc_data_train, 
                          method = "IBCF",
                          parameter = list(k = 24,normalize_sim_matrix=TRUE))

recc_model
class(recc_model)
```
### Applying recommender system on the dataset:

Now, it is time to recommend products to the users in the test set. I define n is equals to 3 which  specifies the number of recommended bank products for each customer.

Then, the IBCF algorithm generates the top 3 recommendation for 1983 users:


```{r apply_IBCF, warning=FALSE, message=FALSE, echo=FALSE}

recc_predicted <- predict(object = recc_model, 
                          newdata = recc_data_test, 
                         n=3)
recc_predicted
```

Let's explore the results of the recommendations for the first user:

```{r explore_res_IBCF, warning=FALSE, message=FALSE, echo=FALSE}

recc_user_1 <- recc_predicted@items[[1]] # recommendation for the first user
item_user_1 <- recc_predicted@itemLabels[recc_user_1]

item_user_1

```

It's also possible to define a matrix with the recommendations for each user. I visualize the recommendations for the first four users:

```{r recc_matrix, warning=FALSE, message=FALSE, echo=FALSE}

recc_matrix <- sapply(recc_predicted@items, function(x){colnames(ratingmat)[x] }) # matrix with the recommendations for each user


as.matrix(recc_matrix)[,1:4]

```

IBCF recommends items on the basis of the similarity matrix. It's an eager-learning model, that is, once it's built, it doesn't need to access the initial data. For each item, the model stores the k-most similar, so the amount of information is small once the model is built. This is an advantage in the presence of lots of data.


```{r times_per_products_IBCF, warning=FALSE, message=FALSE, echo=FALSE}

number_of_items <- table(recc_matrix)

ggplot(as.data.frame(number_of_items), aes(reorder(recc_matrix, -Freq),Freq)) +geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 40, hjust = 1), axis.title.x = element_blank(),  axis.title.y = element_blank())+ggtitle("Distribution of the number of items for IBCF")

```
We see that according IBCF recommender system "Acc_Junior" and "Acc_Saving" are most often recommended items.


In addition, this algorithm is efficient and scalable, so it works well with big rating matrices.

## USER-based Collaborative Filtering Model


In this step, I apply the user-based approach. According to this approach, similar users will be identified  for a new user. Then, the most-popular products owned by similar users are recommended.
For each new user, the steps are following:
  
  *Measure similarity between customers and as  similarity measures use jaccard, pearson correlation, and cosine functions.
  
  *Determine the most similar customers, by taking into account of the top k customers(k-nearest_neighbors) or taking into account the customers whose similarity is above a defined threshold(when we have explicit ratings). 
  
  *Rate the products purchased by the most similar customers.
  
  *Take the top-popular products, which are not owned by customers .


### Building the recommendation system UBCF:

Again, let's first check the default parameters of UBCF model. Here, *nn* is a number of similar users, and *method* is a similarity function, which is *jaccard* by default. I build a recommender model leaving the parameters to their defaults and using the training set.

```{r build_UBCF, warning=FALSE, message=FALSE, echo=FALSE}
recommender_models <- recommenderRegistry$get_entries(dataType ="binaryRatingMatrix")
recommender_models$UBCF_binaryRatingMatrix$parameters

recc_model_u <- Recommender(data = recc_data_train, method = "UBCF", list(k = 30,normalize_sim_matrix=TRUE))
recc_model_u

#names(getModel(recc_model_u))
getModel(recc_model_u)$data
```

### Applying the recommender model on the test set

In the same way as the IBCF, now I determine the top tree recommendations for each new user in the test set. 

```{r apply_UBCF, warning=FALSE, message=FALSE, echo=FALSE}

recc_predicted_u <- predict(object = recc_model_u,
                          newdata = recc_data_test, 
                          n = 3) 
recc_predicted_u
```

Let's take a look at the first user result:

```{r explore_UBCF, warning=FALSE, message=FALSE, echo=FALSE}

recc_user_u1 <- recc_predicted_u@items[[1]] # recommendation for the first user
item_user_u1 <- recc_predicted_u@itemLabels[recc_user_u1]

item_user_u1

```
Also the first ten users:

```{r explore_UBCF_1, warning=FALSE, message=FALSE, echo=FALSE}
recc_matrix_u <- sapply(recc_predicted_u@items, 
                      function(x){ colnames(ratingmat)[x] })

recc_matrix_u[, 1:10]

```

The above matrix contain products name (rows) for the first ten users (columns) in our test data set.

I also compute how many times each products got recommended and build the related frequency histogram:

```{r times_per_products_UBCF, warning=FALSE, message=FALSE, echo=FALSE}

number_of_items <- table(recc_matrix_u)

ggplot(as.data.frame(number_of_items), aes(reorder(recc_matrix_u, -Freq),Freq)) +geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 40, hjust = 1), axis.title.x = element_blank(),  axis.title.y = element_blank())+ggtitle("Distribution of the number of items for UBCF")

```
We see that UBCF recommender system "Acc_Particular", "Acc_e-account" and "Acc_Credit_Card" are most often reccomended items.


## Evaluating the Recommender Systems

There are a few options to choose from when deciding to create a recommendation engine. In order to compare their performances and choose the most appropriate model, I follow these steps:

* Evaluate the performance of some models
* Choose the best performing models
* Optimize model parameters


For evaluation I use the k-fold cross-validation approach. This is the most accurate approach, although its computation is heavier. 

Using k-fold cross-validation approach, it splits data set  into four chunks, take a chunk out as the test set, train on the rest part then test and evaluate the accuracy. The process repeats for other chunks and computes the average accuracy. 


```{r eval_ratings, message=FALSE, warning=FALSE, echo=FALSE}

eval_sets <- evaluationScheme(data = ratingmat, 
                              method = "cross-validation",
                              k = 4, 
                              given = 3,
                              goodRating=1)
size_sets <- sapply(eval_sets@runsTrain, length)
size_sets

eval_recommender <- Recommender(data = getData(eval_sets, "train"),
                                method = "IBCF", 
                                parameter = NULL)

eval_recommender_u <- Recommender(data = getData(eval_sets, "train"),
                                method = "UBCF", 
                                parameter = NULL)

eval_prediction <- predict(object = eval_recommender, 
                           newdata = getData(eval_sets, "known"), 
                           n = 3, 
                           type = "topNList")


eval_prediction_u <- predict(object = eval_recommender_u, 
                           newdata = getData(eval_sets, "known"), 
                           n = 3, 
                           type = "topNList")


```
Using 4-fold approach, we get four sets of the same size 7500.


Now, I compute the accuracy measures for each user and visualize F-measure for each model IBCF and UBCF.

```{r acc, message=FALSE,  warning=FALSE, echo=FALSE}


eval_accuracy <- calcPredictionAccuracy(x = eval_prediction, 
                                        data = getData(eval_sets, "unknown"),byUser=T, given=6)

eval_accuracy_u <- calcPredictionAccuracy(x = eval_prediction_u, 
                                        data = getData(eval_sets, "unknown"), byUser=T, given=6)


head(eval_accuracy)

head(eval_accuracy_u)

qplot((2*eval_accuracy[, "recall"]*eval_accuracy[, "precision"])/(2*eval_accuracy[, "recall"]+eval_accuracy[, "precision"])) + 
  geom_histogram(binwidth = 0.1) + xlab("F-measure")+
  ggtitle("Distribution of the F-measure by user for IBCF")


qplot((2*eval_accuracy_u[, "recall"]*eval_accuracy_u[, "precision"])/(2*eval_accuracy_u[, "recall"]+eval_accuracy_u[, "precision"])) + 
  geom_histogram(binwidth = 0.1) + xlab("F-measure")+
  ggtitle("Distribution of the F-measure by user for UBCF")


```

In order to have a performance index for the whole model, I specify *byUser* as FALSE and compute the average indices:

```{r acc_IBCF, message=FALSE,  warning=FALSE, echo=FALSE}

eval_accuracy <- calcPredictionAccuracy(x = eval_prediction, 
                                        data = getData(eval_sets, "unknown"), 
                                        byUser = F,given=3) 
eval_accuracy["F.measure"]<-2*eval_accuracy[5]*eval_accuracy[6]/(eval_accuracy[5]+eval_accuracy[6])

cat("IBCF: " ,"\n")

eval_accuracy

eval_accuracy_u <- calcPredictionAccuracy(x = eval_prediction_u, 
                                        data = getData(eval_sets, "unknown"), 
                                        byUser = F,given=3) 
eval_accuracy_u["F.measure"]<-2*eval_accuracy_u[5]*eval_accuracy_u[6]/(eval_accuracy_u[5]+eval_accuracy_u[6])

cat( "\n", "UBCF: ", "\n")


eval_accuracy_u


```

The measures of accuracy are useful to compare the performance of different models on the same data.

## Evaluating the recommendations

Another way of evaluating performance. Here, I can make use of a prebuilt
*evaluate* function in *recommenderlab* library. The function evaluate the recommender performance depending on the number *n* of items to recommend to each user. I use *n* as n = seq(1, 10, 1). The first rows of the resulting performance matrix is presented below:

```{r eval_recomms, message=FALSE, warning=FALSE, echo=FALSE}

results <- evaluate(x = eval_sets, 
                    method = "UBCF", 
                    n = seq(1, 10, 1))
getConfusionMatrix(results)[[1]]

```

In order to have a look at all the splits at the same time, I sum up the indices of columns TP, FP, FN and TN:

```{r conf_matrix_whole, message=FALSE, warning=FALSE, echo=FALSE}
columns_to_sum <- c("TP", "FP", "FN", "TN")

indices_summed <- Reduce("+", getConfusionMatrix(results))[, columns_to_sum]

indices_summed

```

Now let's plot the ROC,  the precision/recall, and F-measure curves:

```{r roc, message=FALSE, warning=FALSE}

plot(results, annotate = TRUE, main = "ROC curve")

plot(results, "prec/rec", annotate = TRUE, main = "Precision-recall")

cfm<-as.data.frame(getConfusionMatrix(results)[[1]])

cfm$F.measure<-2*cfm$precision*cfm$recall/(cfm$precision+cfm$recall)

plot(cfm$precision, type="o", col=2,xlab = "number of items reccomended", ylab="index",ylim=c(0,1), main = "Precision, Recall, F-measure")
lines(cfm$recall,type="o", col=3)

lines(cfm$F.measure,type="o", col=4)
legend("topleft", legend = c("Precision", "Recall", "F-measure"), col=2:4, pch=1)


```

If a high number of products are recommended, the precision decreases. On the other hand, the higher percentage of products are recommended the higher is the recall.

## Comparing models

Here I compare different models, I take them as a following list:

* Item-based collaborative filtering(with the Cosine distance function)
* Item-based collaborative filtering(with the Jaccard distance function)
* Item-based collaborative filtering(with the Pearson correlation distance function)
* User-based collaborative filtering(with the Cosine distance function)
* User-based collaborative filtering(with the Jaccard distance function)
* User-based collaborative filtering(with the Pearson correlation distance function)
* Random recommendations(with defaul parametrs)
* Popular recommendations(with defaul parametrs)

```{r define_diff_models, warning=FALSE, message=FALSE, echo=FALSE}

models_to_evaluate <- list(
IBCF_cos = list(name = "IBCF", 
                param = list(method = "cosine")),
IBCF_jac = list(name = "IBCF", 
                param = list(method = "jaccard")),
IBCF_pear = list(name = "IBCF", 
                param = list(method = "pearson")),
UBCF_cos = list(name = "UBCF", 
                param = list(method = "cosine")),
UBCF_jac = list(name = "UBCF", 
                param = list(method = "jaccard")),
UBCF_pear = list(name = "UBCF", 
                param = list(method = "pearson")),
Random = list(name = "Random", 
                param = NULL),
Popular = list(name = "Popular", 
                param = NULL)
)
```

Then, I specify a different set of numbers for recommended products (n_recommendations <- seq(1, 10, 1)), run and evaluate the models:

```{r params, warning=FALSE, message=FALSE, echo=FALSE}

n_recommendations <-seq(1, 10, 1)
list_results <- evaluate(x = eval_sets, 
                         method = models_to_evaluate, 
                         n = n_recommendations)

sapply(list_results, class) == "evaluationResults"

```


## Identifying the best performing model

I compare the models based on a chart displaying their ROC curves and prevision/recall curves.

```{r compare_models_roc, message=FALSE, warning=FALSE, echo=FALSE}
plot(list_results, annotate = 1, legend = "bottomright") 
title("ROC curve")

plot(list_results, "prec/rec", annotate = 1, legend = "topleft" )
title("Precision-recall")
```

According to the Precision/Recall chart we can see that User-Based Collaborative Filtering model gives higher results among the others. Moreover, it gives quit similar results with *Jaccard*, *Cosine* and *Pearson* distances.

Therefore, I choose UBCF with jaccard distance as a best performing model. 


The following table presents as an example of the performance evaluation matrix for the UBCF with Jaccard distance:


```{r ex_compare, warning=FALSE, message=FALSE, echo=FALSE}

avg_matrices <- lapply(list_results, avg)

cfm_avg<-avg_matrices$UBCF_jac[,5:6]

F.measure<-2*cfm_avg[,"precision"]*cfm_avg[,"recall"]/(cfm_avg[,"precision"]+cfm_avg[,"recall"])
cfm_avg<-cbind(cfm_avg, F.measure)

cfm_avg

plot(cfm_avg[,"precision"], type="o", col=2,xlab= "number of items reccomended", ylab="index",ylim=c(0,1), main = "Precision, Recall, F-measure for UBCF method Jaccard distance")
lines(cfm_avg[,"recall"],type="o", col=3)

lines(cfm_avg[,"F.measure"],type="o", col=4)
legend("topleft", legend = c("Precision", "Recall", "F-measure"), col=2:4, pch=1)

```

## Optimizing a numeric parameter for UBCF model

UBCF takes account of the nn nearest_neighbors. I will explore more values, ranging
between 25 and 60, in order to tune this parameter:

```{r optimize, message=FALSE, warning=FALSE}

vector_k <- c(25,30,40,50,60)
models_to_evaluate <- lapply(vector_k, function(k){
  list(name = "UBCF",
       param = list(method = "jaccard", nn = k))
})
names(models_to_evaluate) <- paste0("UBCF_nn_", vector_k)
```

Now I build and evaluate the same UBCF/jaccard models with different values of the nn nearest_neighbors:

```{r eval_optimized, message=FALSE, warning=FALSE, echo=FALSE}

n_recommendations <- seq(1, 10, 1)
list_results <- evaluate(x = eval_sets, 
                         method =  models_to_evaluate, 
                         n = n_recommendations)

plot(list_results, annotate = 1, legend = "bottomright") 
title("ROC curve")

plot(list_results, "prec/rec", annotate = 1, legend = "topleft")
title("Precision-recall")
```

Based on the ROC curve's and the precision/recall plots, with different values of *nn* n_nearest_neighbors we are getting quite similar results. But with *nn* = 60 we get a little bit better results. So, for my model I choose *nn* equal 60. 

I am presenting a simple recommender system created by the *User-Based collaborative approach*. This specific approach was used mainly because it was the best performing method, based on the evaluation performed for this project.

## Conslusions and Discussion

In this project, I have developed and evaluated a collaborative filtering recommender (CFR) system for recommending bank products. 
I built Item-Based and User-Based Collaborative Filtering models with three different similarity measures and tested the performance of each algorithm on the Bank Product ( Binary ) dataset. Based on results, the UBCF model with Jaccard index and nn= 60 gives the best result, when recommended number of products is equal to 3. 

Let's discuss the strengths and weaknesses of the User-based Collaborative Filtering approach in general.

